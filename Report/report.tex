	\documentclass[journal,transmag]{IEEEtran}

\usepackage{cite}
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{IMAGES/}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png,.jpg}
\usepackage{amsmath}
\interdisplaylinepenalty=2500
\usepackage{algorithmic}
\usepackage{array}
\usepackage{booktabs}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfon =sf]{subfig}
\usepackage{dblfloatfix}
\usepackage{url}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{listings}


\lstset{
	escapeinside={/*@}{@*/},
	language=Java,	
	basicstyle=\fontsize{8.5}{12}\selectfont,
	numbers=left,
	numbersep=2pt,    
	xleftmargin=2pt,
	frame=tb,
	columns=fullflexible,
	showstringspaces=false,
	tabsize=4,
	keepspaces=true,
	showtabs=false,
	showspaces=false,
	morekeywords={inline,public,class,private,protected,struct},
	captionpos=b,
	lineskip=-0.4em,
	aboveskip=10pt,
	extendedchars=true,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	keywordstyle=\color[rgb]{0,0,1},
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\color[rgb]{0.627,0.126,0.941},
}

% correct bad hyphenation here
\hyphenation{hy-phen}

\begin{document}
	
	\title{SET10108 Concurrent and Parallel Systems\\Report for Coursework Part 1}
	
	\author{\IEEEauthorblockN{Beej Persson, 40183743@live.napier.ac.uk}
		\IEEEauthorblockA{School of Computing,
			Edinburgh Napier University, Edinburgh}% <-this % stops an unwanted space
		
		\thanks{November 2017}}
	
	
	\markboth{40183743}{}
	% The only time the second header will appear is for the odd numbered pages after the title page when using the twoside option.
	
	\IEEEtitleabstractindextext{
		\begin{abstract}
			For part 1 of the coursework required for the SET10108 Concurrent and Parallel Systems module at Edinburgh Napier University a ray tracing algorithm's performance was to be evaluated and improved by utilising parallel techniques. This report documents one such investigation where the algorithm was parallelised and the difference in its	 performance was measured. 
		\end{abstract}
		
		\begin{IEEEkeywords}
			parallel, ray tracer, \textsc{OpenMP}, C++11, performance, speedup.
		\end{IEEEkeywords}}
	
	\maketitle
	
	\IEEEdisplaynontitleabstractindextext
	
	\IEEEpeerreviewmaketitle
	
	\section{Introduction and Background}
	\IEEEPARstart{T}{he} aim of this report is to evaluate the performance of a ray tracing algorithm and attempt to improve this performance using parallel techniques. The ray tracer initially processes sequentially on a single core of the CPU, but by investigating different parallel techniques the algorithm was changed to run on multiple threads in an attempt to increase the performance.
	
	\subsection{Ray Tracer}
	Ray tracing is a technique used to render an image where "a ray is cast through every pixel of the image, tracing the light coming from that direction" \cite{ray}. The path is traced from an imaginary eye through each pixel on a virtual image plane and the colour of the object visible through it is calculated. It is typically capable of producing visually realistic images of high quality but at a greater computational cost compared to typical rendering techniques. Therefore it tends to be used when an image can be generated slowly ahead of time, but isn't so well suited to the real-time rendering requirements of video games.
	
	\section{Initial Analysis}
	The provided algorithm generates the image by iterating through each pixel using nested for loops. The ray tracer can also sample each pixel multiple times to produce a more accurate and detailed image but at the cost of processing time. Upon running the program a few times and changing the dimensions of the image produced and the number of samples per pixel an idea of its base-line performance was gathered.  By its nature of currently operating sequentially, increasing either the dimensions of the image produced or the number of samples per pixel increases the time it takes to produce the image with an almost perfect positive correlation. That is to say: doubling the size of the image produced or doubling the samples per pixel doubles the time taken.\\
	
	\lstinputlisting[caption = The many nested for loops of the ray tracer algorithm with the operations within the loops removed for clarity.]{./sourceCode/forloop.cpp}
	
	For smaller images at a low number of samples per pixel this results in reasonable times to produce an image, but when producing large images at a high number of samples per pixel the time to produce them was bottlenecked by only running sequentially on a single thread. Most of the time running the program is spent iterating through the nested for loops (seen in Listing 1) and so a clear solution to improving the performance of the algorithm is to run these for loops concurrently on multiple threads.
	
	
	\section{Methodology}	
	A systematic approach was undertaken to evaluate the performance of the algorithm and attempt to measure any improvements in performance gained by parallelising the algorithm.
	
	The first step was to run a series of tests on the provided sequential algorithm to provide a base-line that the performance of the different parallel techniques could be compared to. These tests were all done on the same hardware, the relevant specifications of which are shown in table \ref{hardware}. The details of the tests are shown in the testing subsection below.
	
	\begin{table}[!h]
		\renewcommand{\arraystretch}{1.3}
		\caption{Hardware Specifications}
		\label{hardware}
		\centering
		\begin{tabular}{|l|l|}
			\hline
			Processor & i7-4790K 4 Core HT @ 4.00GHz\\ \hline
			RAM & 16.0GB\\ \hline
			OS & Windows 10 Pro 64-bit\\ \hline
		\end{tabular}
	\end{table}
	
	\subsection{Parallelisation Techniques}
	After these benchmarks for the sequential algorithm were recorded, a few different parallelising techniques were applied to the algorithm and some preliminary tests were run. The intention here was to ensure that the techniques had been implemented correctly and to gain an idea of their relative performance. The techniques used were manual multi-threading and OpenMP with both static and dynamic scheduling.\\
	
	
	\subsubsection{Manual Multi-Threading}
	To parallelise the algorithm using manual multi-threading the set of for loops seen in Listing 1 were added to a single method which could then be run on multiple threads, as shown in Listing 2.
	
	\lstinputlisting[caption = The for loop used to run the forLoopAlgorithm method accross the required number of threads. The variables that are passed to the method are removed for clarity.]{./sourceCode/manualmulti-threading.cpp}
	
	\subsubsection{OpenMP}
	OpenMP is an API that supports shared-memory parallel programming and allows some additional manipulations in the scheduling that were used in an attempt to increase performance. The pre-processor argument shown in Listing 3 was used to parallelise the outer for loop, allowing the algorithm to be run across multiple threads.
	
	\lstinputlisting[caption = The OpenMP parallel for used to parallelise the shown for loop across the number of threads desired. The removed nested for loops can be seen in Listing 1.]{./sourceCode/omp.cpp}
	
	OpenMP's parallel for function comes with a \textit{schedule} clause, seen in Listing 3, that can be used to change the way it spreads the workload across the threads. By default, OpenMP statically assigns each for loop iteration to a thread. However, if each iteration takes a different amount of time, it can be beneficial to use dynamic scheduling. When scheduled dynamically the threads can request work when ready and be assigned the next iteration that hasn't been executed yet. Given that this may further improve the performance of the ray tracing algorithm, both types of scheduling were tested.
	
	\subsection{Testing}
	The same series of tests that were run on the sequential algorithm were then undertaken for each implemented parallelisation. These tests were done under the same conditions and on the same hardware to eliminate discrepancies. The testing parameters used to evaluate the performance of the algorithms is outlined below.
	
	The dependent variable being measured was the amount of time it took for the algorithms to produce all the data in the pixel vector, which is used to generate the final image. The independent variables were the dimensions of the image and the number of samples per pixel. First the image size was kept constant at 256x256 whilst the number of samples per pixel was incremented, by powers of 2, from 4 up to 512. After this the samples per pixel was kept constant at 16 whilst the dimensions of the image were incremented, again by powers of 2, from 128x128 up to 1024x1024. For each change in the independent variables, 100 tests were run and the time it took for the algorithm to produce the data recorded, before the average run times were calculated. Further to this, 2 tests were run with a large image size of 1024x1024 and 1024 samples per pixel. This was only done twice due to how long it took for the sequential algorithm to generate the image, but was still useful for comparison. Further to this, a few additional tests were done on each parallel algorithm where the number of threads they ran on were controlled. This was done to help contextualise whether the potential performance increase came from the number of additional threads or from changes to the algorithm itself.
	
	\subsection{Evaluation}
	The results of these tests were then collated and compared to the results from the sequential algorithm's testing and used as the basis for the evaluation of	their respective performance.
	
	To represent the improved performance, the efficiency, $E$, of the algorithms was calculated using the formula shown in Equation \ref{efficiency} below:
	
	\begin{equation} \label{efficiency} 
	E = \dfrac{S}{p} = \dfrac{(\dfrac{T_{serial}}{T_{parallel}})}{p}
	\end{equation}
	
	Where $S$ is speedup, $p$ is the number of processor cores, and $T_{serial}$ and $T_{parallel}$ are the sequential and parallel computational times respectively.
	
	\section{Results and Discussion}
	The results from the testing done on the algorithms can be seen summarised in tables \ref{comptable1} below. As discussed in the initial analysis there is an almost perfect positive correlation between the average time and the increasing samples per pixel and image dimensions for the sequential algorithm, but this also holds true for parallel algorithms.
	
	\begin{table}[!h]
		\caption{256x256 Image Generation Performance Comparison}
		\label{comptable1}
		\centering
		\resizebox{\columnwidth}{!}{%
		\begin{tabular}{@{}ccccc@{}}
			\toprule
			\multicolumn{5}{c}{Image Dimensions 256x256} \\ \midrule
			\multicolumn{1}{c|}{Algorithm} & \multicolumn{1}{c}{Sequential} & \multicolumn{1}{c}{OpenMP Static} & \multicolumn{1}{c}{OpenMP Dynamic} & \multicolumn{1}{c}{Manual Multi-Threading} \\ \midrule
			\multicolumn{1}{c|}{Samples Per Pixel} & \multicolumn{4}{c}{Average Time / ms} \\ \midrule
			\multicolumn{1}{c|}{4} & 1.0 & \multicolumn{1}{c|}{1.0} & 1.7 & 0.989 \\
			\multicolumn{1}{c|}{8} & 1.0 & \multicolumn{1}{c|}{1.0} & 2.5 & 0.920 \\
			\multicolumn{1}{c|}{16} & 1.0 & \multicolumn{1}{c|}{1.0} & 3.5 & 0.919 \\
			\multicolumn{1}{c|}{32} & 1.0 & \multicolumn{1}{c|}{1.0} & 3.4 & 0.901 \\
			\multicolumn{1}{c|}{64} & 1.0 & \multicolumn{1}{c|}{1.0} & 25.5 & 0.962 \\
			\multicolumn{1}{c|}{128} & 1.0 & \multicolumn{1}{c|}{1.0} & 82.9 & 0.949 \\
			\multicolumn{1}{c|}{256} & 1.0 & \multicolumn{1}{c|}{1.0} & 183.6 & 0.968 \\
			\multicolumn{1}{c|}{512} & 1.0 & \multicolumn{1}{c|}{1.0} & 335.1 & 0.968 \\ \bottomrule
		\end{tabular}%
		}
	\end{table}
	
	A graph comparing the average time taken for the algorithms to generate the data required to produce a 256x256 image at different numbers of samples per pixel is shown below in Figure \ref{fig1}.
	
	\begin{figure}[!h]
		\centering
		\includegraphics[width=\columnwidth]{IMAGES/performancecomparison1}
		\caption{A graph showing the average time it took each algorithm to generate a 256x256 image at different numbers of samples per pixel.}
		\label{fig1}
	\end{figure}
	
	As the number of samples per pixel were incremented in powers of 2, the data is discrete and the $x$-axis has been displayed using a base-2 logarithmic scale. As a result of this the $y$-axis is also displayed using a base-2 logarithmic scale so that data does not appear skewed. Here we can see that all the parallel algorithms show significant speed up over the sequential algorithm, with the manual multi-threading being the fastest.

	
	\begin{figure}[!h]
		\centering
		\includegraphics[width=\columnwidth]{IMAGES/performancecomparison2}
		\caption{A graph showing the average time it took each algorithm to generate images of varying dimensions at 16 samples per pixel.}
		\label{fig2}
	\end{figure}


	\begin{figure}[!h]
		\centering
		\includegraphics[width=\columnwidth]{IMAGES/singlethreadperformance}
		\caption{A graph showing the average time it took each algorithm to generate a 256x256 image at 16 samples per pixel whilst limited to a single thread.}
		\label{fig3}
	\end{figure}

	Suitable performance analysis and testing documentation
	for the problem, including quality of presentation of
	the results. [10]
	
	\section{Conclusion}
	Level of discussion and appropriateness of the conclusions
	drawn based on the results gathered. [10]
	
	\newpage
	
	\appendices
	\section{Proof of fucking something}
	Appendix one text goes fucking here.
	\lstinputlisting[caption = A fucking code listing.]{./sourceCode/hello.cpp}
	
	\section{}
	Fucking appendix two text goes here.
	
	\bibliographystyle{IEEEtran}
	\bibliography{bibliography}
	
\end{document}