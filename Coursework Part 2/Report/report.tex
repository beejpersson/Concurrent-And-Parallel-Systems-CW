\documentclass[12pt,journal,transmag]{IEEEtran}

\usepackage{cite}
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{IMAGES/}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png,.jpg}
\usepackage{amsmath}
\interdisplaylinepenalty=2500
\usepackage{algorithmic}
\usepackage{array}
\usepackage{booktabs}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfon =sf]{subfig}
\usepackage{dblfloatfix}
\usepackage{url}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{listings}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\onehalfspacing


\lstset{
	escapeinside={/*@}{@*/},
	language=Java,	
	basicstyle=\fontsize{8.5}{12}\selectfont,
	numbers=left,
	numbersep=2pt,    
	xleftmargin=2pt,
	frame=tb,
	columns=fullflexible,
	showstringspaces=false,
	tabsize=4,
	keepspaces=true,
	showtabs=false,
	showspaces=false,
	morekeywords={inline,public,class,private,protected,struct},
	captionpos=b,
	lineskip=-0.4em,
	aboveskip=10pt,
	extendedchars=true,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	keywordstyle=\color[rgb]{0,0,1},
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\color[rgb]{0.627,0.126,0.941},
}

% correct bad hyphenation here
\hyphenation{hy-phen}

\begin{document}
	
	\title{SET10108 Concurrent and Parallel Systems\\Report for Coursework Part 2}
	
	\author{\IEEEauthorblockN{Beej Persson, 40183743@live.napier.ac.uk}
		\IEEEauthorblockA{School of Computing,
			Edinburgh Napier University, Edinburgh}% <-this % stops an unwanted space
		
		\thanks{December 2017}}
	
	
	\markboth{40183743}{}
	% The only time the second header will appear is for the odd numbered pages after the title page when using the twoside option.
	
	\IEEEtitleabstractindextext{
		\begin{abstract}
			For part 2 of the coursework required for the SET10108 Concurrent and Parallel Systems module at Edinburgh Napier University an \textit{n}-body simulation application's performance was to be evaluated and improved by utilising parallel techniques. This report documents one such investigation where the algorithm was parallelised and the difference in its performance was measured. 
		\end{abstract}
		
		\begin{IEEEkeywords}
			parallel, \textit{n}-body, OpenMP, CUDA, C++11, performance, speedup, efficiency.
		\end{IEEEkeywords}}
	
	\maketitle
	
	\IEEEdisplaynontitleabstractindextext
	
	\IEEEpeerreviewmaketitle
	
	\section{Introduction and Background}
	\IEEEPARstart{T}{he} aim of this report is to evaluate the performance of an \textit{n}-body application and attempt to improve this performance using parallel techniques. The \textit{n}-body algorithm used initially processes sequentially on a single core of the CPU, but by investigating different parallel techniques the algorithm was changed to run on either multiple CPU cores or the GPU in an attempt to increase the performance.
	
	\subsection{N-body Problem}
	The \textit{N}-body problem is the problem of attempting to predict the positions and velocities of a group of bodies whilst they interact with each other via gravity. Finding a solution to this problem is generally done by calculating the sum of the forces acting on a each body in the system and using this to estimate its velocity and position. Given a body's mass, $m\textsubscript{$i$}$, and position, $p\textsubscript{$i$}$, the force acting on it by another body, $m\textsubscript{$j$}$ and $p\textsubscript{$i$}$ is given by Equation \ref{forceeq} below (Reference \cite{meyer}):
	
	\begin{equation} \label{forceeq} 
	F\textsubscript{$ij$} = \dfrac{Gm\textsubscript{$i$}m\textsubscript{$j$}(p\textsubscript{$j$} - p\textsubscript{$i$})}{||p\textsubscript{$j$} - p\textsubscript{$i$}||\textsuperscript{$3$}}
	\end{equation}
	
	Where $F$ is the force and $G$ is the gravitational constant. Using this equation, and knowing that $F = ma$, the acceleration of the body can be determined. Thus the new velocity and position can be found by multiplying the acceleration by a chosen timestep. To produce an \textit{n}-body simulation this calculation can be done multiple times with a small enough timestep to accurately model the movement of the bodies in a space.
	
	\subsection{N-body Simulation}
	The application used to generate an n-body simulation was written in C++ using a combination of two \textit{n}-body algorithms available online. The structure of the application was based on Mark Harris'\cite{harris}, whilst much of the maths used to calculate forces was based on Mark Lewis'\cite{lewis}. To ensure the algorithm operated as intended, the simulation was visualised by generating a data file of the body's positions and radii at each timestep and running a python script that converted that data into a video file.
	
	\section{Initial Analysis}
	Upon running the application a few times and changing the number of bodies and the number of iterations of the simulation, an idea of its baseline performance was gathered. Below, in Tables \ref{table1} and \ref{table2}, the results of this initial testing on the sequential algorithm can be seen.
	
	\begin{table}[!h]
		\caption{1000 Iteration Sequential Algorithm Performance}
		\label{table1}
		\centering
		\resizebox{\columnwidth}{!}{%
			\begin{tabular}{cc}
				\toprule
				\multicolumn{2}{c}{Simulation Iterations = 1000} \\ \midrule
				\multicolumn{1}{c|}{Number of Bodies} & \multicolumn{1}{c}{Average Time / ms} \\ \midrule
				\multicolumn{1}{c|}{64} & 20.3 \\
				\multicolumn{1}{c|}{128} & 80.87 \\
				\multicolumn{1}{c|}{256} & 322.96\\
				\multicolumn{1}{c|}{512} & 1289.07 \\
				\multicolumn{1}{c|}{1024} & 5119.45 \\
				\multicolumn{1}{c|}{2048} & 20196.89 \\
				\multicolumn{1}{c|}{4096} & 80797.32 \\ \bottomrule
			\end{tabular}%
		}
	\end{table}

	\begin{table}[!h]
		\caption{1024 bodies Sequential Algorithm Performance}
		\label{table2}
		\centering
		\resizebox{\columnwidth}{!}{%
			\begin{tabular}{cc}
				\toprule
				\multicolumn{2}{c}{Number of Bodies = 1024} \\ \midrule
				\multicolumn{1}{c|}{Simulation Iterations} & 
				\multicolumn{1}{c}{Average Time / ms} \\ \midrule
				\multicolumn{1}{c|}{250} & 1337.40 \\
				\multicolumn{1}{c|}{500} & 2699.15 \\
				\multicolumn{1}{c|}{750} & 3884.60 \\
				\multicolumn{1}{c|}{1000} & 5214.85 \\
				\multicolumn{1}{c|}{1250} & 6523.30 \\
				\multicolumn{1}{c|}{1500} & 7799.55 \\ \bottomrule
			\end{tabular}%
		}
	\end{table}
	
	The application's $calcForces$ method had complexity of O($n\textsuperscript{2}$): it contained a nested forloop, where each body in the system was compared against every other body. Given this, increasing the number of bodies results in the time taken to run 1000 iterations of the simulation to increase at an $n^2$ rate, as seen in Table \ref{table1}. However, running more simulation iterations resulted in a linear increase in the time taken to simulate 1024 bodies, with almost perfect positive correlation, as can be seen in Table \ref{table2}.
	
	Further to this, a performance profiler was run to identify the possible bottlenecks and determine the best areas to attempt parallelisation to improve the application's performance. As can be seen in the code's hot path in Figure \ref{figure1}, the \textit{calcForces} method, discussed earlier, was what took up the majority of processing time when there was a significant number of bodies.
	
	\begin{figure}[!h]
		\centering
		\includegraphics[width=\columnwidth]{IMAGES/hotpath}
		\caption{A image showing the results of running Visual Studio's Performance Profiler, where the "Hot Path" is displayed.}
		\label{figure1}
	\end{figure}
	
	This method, therefore, was the area that was parallelised in attempt to improve performance. However, when there was only 2 bodies the method executed very quickly showing that each individual comparison between bodies required little processing time. The high processing time when there was a large number of bodies was simply that there were so many comparisons to be made. This opened up GPU parallelisation as a potential solution given the large number of lower clock-speed cores.
	
	\section{Methodology}
	
	A systematic approach was undertaken to evaluate the performance of the algorithm and to measure any improvements in performance gained by parallelising the algorithm. The parallelising technologies were chosen based on the results of the initial analysis and on their ability to maximise the performance of the application.
	
	The first step was to run a series of tests on the application to determine likely areas that could be parallelised and which technologies would be suitable, and to provide a baseline that the performance of the different parallel implementations could be compared to. These tests were all done on the same hardware, the relevant specifications of which are shown in table \ref{hardware}. The details of the tests are shown in the testing subsection below.
	
	\begin{table}[!h]
		\renewcommand{\arraystretch}{1.3}
		\caption{Hardware Specifications}
		\label{hardware}
		\centering
		\begin{tabular}{r|l}
			\toprule
			CPU & i7-4790K 4 Core HT @ 4.00GHz\\ \hline
			GPU & NVIDIA GTX 980 @ 1.12MHz \\ \hline
			RAM & 16.0GB\\ \hline
			OS & Windows 10 Pro 64-bit\\ \bottomrule
		\end{tabular}
	\end{table}
	
	\subsection{Parallelisation Techniques}
	After these benchmarks for the sequential algorithm were recorded, the chosen parallelising techniques were applied to the algorithm and some preliminary simulations were run, each time checking the visualised output of the application. The intention here was twofold; to ensure that the techniques had been implemented correctly, that the parallelised algorithm was still producing the same simulation as the sequential application, and to gain an idea of their relative performance. The techniques used were OpenMP and CUDA, and the parallelisation was only applied to the $calcForces$ method as it took the majority of processing time and in an attempt to reduce accidental speedup to the application beyond simply parallelising the sequential algorithm itself.
	
	\subsubsection{OpenMP}
	OpenMP is an API that supports shared-memory parallel programming and allows some additional manipulations in the scheduling that were used in an attempt to increase performance. The pre-processor argument shown in Listing 1 was used to parallelise the outer forloop, allowing the force calculation algorithm to be run across multiple threads.
	
	\lstinputlisting[caption = The OpenMP parallel for used to parallelise the shown forloop across the number of threads desired.]{./sourceCode/omp.cpp}
	
	OpenMP's parallel for function comes with a \textit{schedule} clause, seen in Listing 1, that was set to static as each iteration of the loop took the same amount of time to process. OpenMP was chosen as a parallel technique as the initial analysis showed that the $calcForces$ method was taking a long time to compute when run sequentially, and parallelising this algorithm so that it could run on multiple threads simultaneously would improve the performance of the application. OpenMP was chosen as it provides this parallelisation simply and effectively across the available CPU cores.
	
	\subsubsection{CUDA}
	CUDA is an API and parallel computing platform created by NVIDIA that allows software to utilise a CUDA-enabled GPU's virtual instruction set and execute the application in parallel using compute kernels. CUDA allows the user to determine the number of \textit{blocks} and \textit{threads per block}, showing in Listing 2, to be used for the kernel method and some testing was done to determine the optimal ratios. Below is an excerpt from the CUDA application.
	
	\lstinputlisting[caption = The $calcForces$ method with CUDA adjustments made and how the kernel method is called within the $main$.]{./sourceCode/cuda.cu}
	
	CUDA was chosen as a notable contrast to OpenMP as it runs on the GPU and would enable a comparison between their respective performance. Further to this, given the results of the initial analysis, executing the $calcForces$ method in parallel on the GPU would provide significant speedup, especially for larger numbers of particles, due to the large number of available cores.
	
	\subsection{Testing}
	The same series of tests that were run on the sequential algorithm in the initial analysis were then undertaken for each implemented parallelisation. These tests were done under the same conditions and on the same hardware to eliminate discrepancies. For the majority of the OpenMP tests, the algorithm was run on the maximum number of threads available. CUDA allowed effective cores in use to be determined as attributes in the kernel method, therefore some initial testing was done on the performance of the application with the number of \textit{threads per block} manipulated to identify the number of \textit{blocks} and \textit{threads per block} that would be used in the later tests.
	
	The testing parameters used to evaluate the performance of the algorithms and the tests themselves are listed below.
	
	For all tests the dependent variable being measured was the amount of time it took for the applications to produce the required number of iterations of the simulation. For each change in the independent variables, 100 tests were run and the time it took for the algorithm to produce the necessary values recorded, before the average run times were calculated. 
	
	\begin{enumerate}
		\item CUDA's Threads Per Block
		\begin{itemize}
			\item 	Independent variables: threads per block (from 1 to 32 incremented by powers of 2).
			\item 	Constants: simulation iterations (1000), number of bodies (1024).
		\end{itemize}
		\item CUDA and OpenMP Comparisons
		\begin{itemize}
			\item 	Independent variables: number of bodies (from 64 to 4096 incremented by powers of 2)
			\item 	Shared constants: simulation iterations (1000).
			\item	CUDA constants: threads per block (8), blocks (number of bodies divided by threads per block).
			\item OpenMP constants: threads (8).
		\end{itemize}
		\item Single Thread Tests
		\begin{itemize}
			\item 	Independent variables: each algorithm (sequential, CUDA, OpenMP).	
			\item 	Constants: simulation iterations (1000), number of bodies (512), threads (1).
		\end{itemize}
	\end{enumerate}
	
	\subsection{Evaluation}
	The results of these tests were then collated and compared to the results from the sequential algorithm's testing and used as the basis for the evaluation of their respective performance.
	
	To represent the improved performance, the speedup, $S$, and efficiency, $E$, of the algorithms was calculated using the formula shown in Equation \ref{efficiencyeq} below:
	
	\begin{equation} \label{efficiencyeq} 
		S = \dfrac{T_{serial}}{T_{parallel}}, \qquad E = \dfrac{S}{p}
	\end{equation}
	
	Where $T_{serial}$ and $T_{parallel}$ are the sequential and parallel computational times respectively, and $p$ is the number of processor cores. When calculating the efficiency of the OpenMP application, $p$ was set to 4, which was the number of available CPU cores on the test hardware. However, determining $p$ for the CUDA application was more complex. Whilst the GPU used to test the application was listed as having 2048 cores, that wasn't necessarily the number of hardware cores assigned when running the kernel method. Instead, $p$ was calculated as the number of \textit{blocks} multiplied by the number of \textit{threads per block} set as attributes in the kernel method, as this results in the total number of threads doing work on the GPU.
	
	\section{Results and Discussion}
	
	The results of the initial test on the CUDA application can be found in Table \ref{cudathreadtable} below.
	
	\begin{table}[!h]
		\caption{CUDA Performance: Threads Per Block}
		\label{cudathreadtable}
		\centering
		\resizebox{\columnwidth}{!}{%
			\begin{tabular}{cc}
				\toprule
				\multicolumn{2}{c}{Number of Bodies = 1024, Simulation Iterations = 1000} \\ \midrule
				\multicolumn{1}{c|}{Threads Per Block} & Average Time / ms \\ \midrule
				\multicolumn{1}{c|}{1} & 3126.17 \\
				\multicolumn{1}{c|}{2} & 1183.69 \\
				\multicolumn{1}{c|}{4} & 1051.98 \\
				\multicolumn{1}{c|}{8} & 1058.97 \\
				\multicolumn{1}{c|}{16} & 1117.77 \\
				\multicolumn{1}{c|}{32} & 1256.76\\ \bottomrule
			\end{tabular}%
		}
	\end{table}

	When the kernel method was called with only a single \textit{thread per block}, the application took significantly longer to generate a 1000 iterations of the simulation of 1024 bodies than when using a higher number of  \textit{threads per block}. Whilst there was a noticeable increase in performance when run using 2 \textit{threads per block}, the average time seemed to reach its lowest at around 4 or 8 \textit{threads per block}, before again increasing at 16 or more \textit{threads per block}. These results are visualised in Figure \ref{graph1} below. The error bars on this graph were determined by calculating the standard deviations of each set of 100 test results, which, as can be seen, were very small.

	\begin{figure*}[!h]
		\centering
		\includegraphics[width=1.5\columnwidth]{IMAGES/cudathreads}
		\caption{A graph showing the average time it took the CUDA application to generate the simulation data when changing the allocated number of threads per block on the GPU.}
		\label{graph1}
	\end{figure*}

	Given these results, 8 \textit{threads per block} were used for all CUDA performance tests undertaken afterwards, and \textit{blocks} were set to the number of bodies being tested divided by \textit{threads per block}, to ensure each body would receive its own thread, maximising use of the available hardware.
	
	The results from the performance testing done on the applications can be seen summarised in Table \ref{comptable} below. 

	\begin{table}[!h]
		\caption{1000 Iterations N-Body Simulation}
		\label{comptable}
		\centering
		\resizebox{\columnwidth}{!}{%
			\begin{tabular}{cccc}
				\toprule
				\multicolumn{4}{c}{Number of Iterations = 1000} \\ \midrule
				\multicolumn{1}{c|}{Application} & \multicolumn{1}{c}{Sequential} & \multicolumn{1}{c}{OpenMP} & \multicolumn{1}{c}{CUDA} \\ \midrule
				\multicolumn{1}{c|}{Number of Bodies} & \multicolumn{3}{c}{Average Time / ms} \\ \midrule
				\multicolumn{1}{c|}{64} & 20.39 & 15.46 & 105.75 \\
				\multicolumn{1}{c|}{128} & 80.87 & 45.81 & 156.79 \\
				\multicolumn{1}{c|}{256} & 322.96 & 148.66 & 285.48 \\
				\multicolumn{1}{c|}{512} & 1289.07 & 533.66 & 528.34 \\
				\multicolumn{1}{c|}{1024} & 5119.45 & 2007.81 & 1063.36 \\
				\multicolumn{1}{c|}{2048} & 20196.89 & 7820.01 & 2398.78 \\
				\multicolumn{1}{c|}{4096} & 80797.32 & 30628.95 & 6822.16 \\ \bottomrule
			\end{tabular}%
		}
	\end{table}

	As discussed in the initial analysis there is an $n^2$ positive correlation between the average time and the increasing number of bodies for the sequential application, but this also holds true for the OpenMP application. For the CUDA application, however, the relationship is more complicated. For a small number of bodies, the average time taken to generate an \textit{n}-body simulation was higher than both the OpenMP and sequential applications. But as the number of bodies became increasingly large, the average times became significantly lower than the other applications. For both parallel applications, whenever there was a more significant number of bodies being simulated, the average times in which they produced the data were much lower than the sequential application.
	
	\begin{figure*}[!h]
		\centering
		\includegraphics[width=1.5\columnwidth]{IMAGES/perfcomp1}
		\caption{A graph showing the average time it took each application to generate 1000 iterations of a simulation with differing numbers of bodies.}
		\label{graph2}
	\end{figure*}
	
	Below, in Figure \ref{graph2}, is the graph of these results. The error bars on this graph were once again determined by calculating the standard deviations of each set of 100 test results and are, again, barely visible. As the number of bodies to be simulated were incremented in powers of 2, the data is discrete and the $x$-axis has been displayed using a base-2 logarithmic scale. As a result of this the $y$-axis is also displayed using a base-2 logarithmic scale so that data does not appear skewed.
	
	By running these averaged times through the formula shown in Equation \ref{efficiencyeq}, the speedup and efficiencies of these applications compared against the sequential application were calculated. Table \ref{efficiencytable} below shows these results.

	\begin{table}[!h]
		\caption{Algorithmic Speedup and Efficiency Comparison}
		\label{efficiencytable}
		\centering
		\resizebox{\columnwidth}{!}{%
			\begin{tabular}{@{}ccccc@{}}
				\toprule
				\multicolumn{5}{c}{Number of Iterations = 1000} \\ \midrule
				\multicolumn{1}{c|}{Application} & OpenMP & \multicolumn{1}{c|}{CUDA} & OpenMP & CUDA \\ \midrule
				\multicolumn{1}{c|}{Number of Bodies} & \multicolumn{2}{c|}{Speedup} & \multicolumn{2}{c}{Efficiency} \\ \midrule
				\multicolumn{1}{c|}{64} & 1.319 & \multicolumn{1}{c|}{0.193} & 0.330 & 0.024 \\
				\multicolumn{1}{c|}{128} & 1.765 & \multicolumn{1}{c|}{0.516} & 0.441 & 0.032 \\
				\multicolumn{1}{c|}{256} & 2.172 & \multicolumn{1}{c|}{1.131} & 0.543 & 0.035 \\
				\multicolumn{1}{c|}{512} & 2.416 & \multicolumn{1}{c|}{2.440} & 0.604 & 0.038 \\
				\multicolumn{1}{c|}{1024} & 2.550 & \multicolumn{1}{c|}{4.814} & 0.637 & 0.038 \\
				\multicolumn{1}{c|}{2048} & 2.583 & \multicolumn{1}{c|}{8.420} & 0.646 & 0.033 \\
				\multicolumn{1}{c|}{4096} & 2.638 & \multicolumn{1}{c|}{11.843} & 0.659 & 0.023 \\ \bottomrule
			\end{tabular}%
		}
	\end{table}

	This table and its accompanying graphs, Figures \ref{graph3} and \ref{graph4}, show that for low numbers of bodies, OpenMP bested CUDA in terms of both speedup and efficiency when compared to the sequential application. When generating a simulation with 512 bodies or more, however, CUDA significantly outperformed OpenMP in terms of speedup, and was only improving as more and more bodies were simulated. In spite of these speedups, CUDA remained incredibly inefficient throughout, whilst OpenMP was nearly 3 times more efficient than the sequential application. 

	\begin{figure*}[!h]
		\centering
		\includegraphics[width=1.6\columnwidth]{IMAGES/speedupcomp}
		\caption{A graph showing the speedup of the different parallel applications compared to the sequential application when generating 1000 iterations of a simulation with differing numbers of bodies.}
		\label{graph3}
	\end{figure*}
	
	\begin{figure*}[!h]
		\centering
		\includegraphics[width=1.6\columnwidth]{IMAGES/effcomp}
		\caption{A graph showing the efficiency of the different parallel applications compared to the sequential application when generating 1000 iterations of a simulation with differing numbers of bodies.}
		\label{graph4}
	\end{figure*}

	\begin{figure*}[!h]
		\centering
		\includegraphics[width=1.5\columnwidth]{IMAGES/singlethread}
		\caption{A graph showing the average time it took each application to generate 1000 iterations of a simulation with 512 bodies whilst limited to a single thread.}
		\label{graph5}
	\end{figure*}	

	The positive error for speedup was determined by calculating the maximum sequential time divided by the minimum parallel time for the relevant number of bodies using their respective standard deviation error, which provides the maximum theoretical speedup. The reverse was done to determine the negative error. The speedups' maximums and minimums were then used to determine the maximum and minimum efficiency in a similar vein, which were used as their error bars. These calculated error bars are visible with a lower number of bodies, but become less significant when more bodies are added to the simulation, as is expected.
	
	As discussed in the testing section earlier, a single threaded performance test was also performed to help contextualise these efficiencies. The results of these can be seen below in Table \ref{singlethreadtable} and visualised in \ref{graph5}. The error bars in this graph were determined by calculating the standard deviations of the test results, as before. 

	\begin{table}[!h]
		\caption{Single Thread Performance Comparison}
		\label{singlethreadtable}
		\centering
		\resizebox{\columnwidth}{!}{%
			\begin{tabular}{@{}cc@{}}
				\toprule
				\multicolumn{2}{c}{Iterations = 1000, No. of Bodies = 512} \\ \midrule
				\multicolumn{1}{c|}{Application} & Average Time / ms \\ \midrule
				\multicolumn{1}{c|}{Sequential} & 1289.07 \\
				\multicolumn{1}{c|}{OpenMP} & 1273.42 \\
				\multicolumn{1}{c|}{CUDA} & 112779.60 \\ \bottomrule
			\end{tabular}%
		}
	\end{table}

	When limited to operating on a single thread the OpenMP application produced the simulation data faster than the sequential application showing that there were some incidental optimisations being implemented when OpenMP was used, shown in Figure \ref{graph5}. However, when the CUDA application was limited to a single thread, by setting both \textit{blocks} and \textit{threads per block} to 1 in the kernel method call, the data for the simulation took a vast amount of time to be generated. The CUDA application took longer to produce the data to simulate just 512 bodies than the sequential application took for 4096 bodies, shown in Table \ref{table1}. These results help to better understand why CUDA's speedups were so high, whilst its efficiency remained low.
	
	\section{Conclusion}
	\subsection{Explanation of Results and Evaluation of Performance}
	Across all the tested applications, when the number of bodies to be simulated was increased, the time it took to produce the data used to generate the simulation would increase with an $n^2$ positive correlation. Furthermore, as shown in Figure \ref{graph2}, when there was a small number of bodies the OpenMP application performed slightly better than the sequential application whilst the CUDA application got outperformed by both. However, when simulating a larger and larger number of bodies, the CUDA application's performance would begin to outshine the others.
	
	The OpenMP application had a speedup of up to 3 when compared against the sequential algorithm, particularly when simulating a larger number of bodies, as seen in Figure \ref{graph3}. When only generating the data to simulate a few bodies, its speedup was closer to 1. This was likely due to the fact that the average time to simulate such a small number of bodies was low, even for the sequential algorithm, and the additional time the OpenMP application would take to initialise its threads becomes a more significant factor. As soon as there was a larger amount of time spent within the parallelised $calcForces$ method, the OpenMP application's performance would increase. The CUDA application, on the other hand, had speedups of less than 1 when simulating a small number of bodies. The time taken to pass the initial \textit{n}-body data from the CPU to the GPU and the time taken to pass the calculated forces back to the CPU from the GPU was a significant aspect of this. For these small numbers of bodies, the time spent inside the $calcForces$ kernel method was less than the time spent passing the data back and forth between the host and the device, resulting in processing times larger than the sequential application. When producing data to simulate a much larger number of bodies, however, the CUDA application would produce massive speedups of up to 12 (Figure \ref{graph3}) over the sequential application. The longer that was needed to calculate the forces to be applied to all the bodies, the better this application would perform in comparison to the others.
	
	The efficiencies of the OpenMP application were expected, each core was less efficient than the sequential application's single core, but when run in parallel the overall performance increase was noticeable. The efficiencies of the CUDA application, on the other hand, were more interesting. Even when producing the data to simulate 4096 bodies, where its speedup was almost 12, each individual core of the GPU had efficiencies of around 0.03, when compared to the sequential application's single core, as shown in Figure \ref{graph5}. When limited to a single thread the reason for this low efficiency was highlighted. It took a single thread on the GPU roughly a hundred times longer to produce the same data as the sequential application did on the CPU. This was likely as a result of the significantly lower clock speed of the GPU's cores compared against the CPU, shown in the hardware specifications, Table \ref{hardware}. Therefore, when the CUDA application was generating the data required for a large number of bodies, the kernel method was called using values of 8 for the \textit{threads per block} and the number of bodies divided by 8 for the \textit{blocks}, which created a large number of available cores resulting in very low efficiencies. The number of cores used to calculate the efficiency on the GPU was \textit{blocks} multiplied by \textit{threads per block}, which resulted in the value of $p$ being equal to the number of bodies being simulated. Therefore, whilst the average time to produce that much data was lower than both the sequential application and the OpenMP application, and its speedup much higher, it was always significantly less efficient.
	
	\subsection{Final Thoughts}
	The written sequential algorithm originally took a considerable amount of time to produce \textit{n}-body simulations with a large number of bodies. The attempts at parallelising the algorithm were largely successful. They both performed significantly faster than the sequential application when working with a large number of bodies and made the time to generate the data for long simulations of those bodies more manageable. However, for both parallelisation techniques, when simulating a smaller number of bodies, the improvements were either minor or non-existent. As expected, when parallelised across the CPU, the speedup was noticeable with only a slight loss in efficiency, whilst when parallelised across the GPU, the speedup was much more impressive but each individual thread was very inefficient. In terms of increased performance when desiring a long simulation of a large number of bodies, in comparison to the original sequential application, the CUDA parallelisation produced the most notable improvement.
	
	\bibliographystyle{IEEEtran}
	\bibliography{bibliography}
	
\end{document}